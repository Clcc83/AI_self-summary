<html>
<head>
  <title>Chapter 7.Ensemble Learning and Random Forests</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/607057 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1059"/>
<h1>Chapter 7.Ensemble Learning and Random Forests</h1>

<div><span><h1 style="text-align:center;">Chapter 7.<span style="font-weight: bold;">Ensemble Learning and Random  Forests</span></h1><div> </div><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div> 假设你去随机问很多人一个很复杂的问题，然后把它们的答案合并起来。通常情况下你会发现这个合并的答案比一个专家的答案要好。这就叫做群体智慧。同样的，如果你合并了<span style="background-color: #fff199;">一组分类器的预测</span>（像分类或者回归），你也会得到一个比单一分类器更好的预测结果。这一组分类器就叫做集成；因此，这个技术就叫做集成学习，一个集成学习算法就叫做集成方法。</div></div><p><br/></p><p>例如，你可以训练一组决策树分类器，每一个都在一个随机的训练集上。为了去做预测，你必须得到所有单一树的预测值，然后通过投票来预测类别。例如一种决策树的集成就叫做随机森林，它除了简单之外也是现今存在的最强大的机器学习算法之一。</p><p><br/></p><div>在本章中我们会讨论一下特别著名的集成方法，包括 bagging, boosting, stacking，和其他一些算法。我们也会讨论随机森林。</div><div><br/></div><h3>一、Voting Classifiers</h3><div><span style="font-size: 14pt;">1、Hard Voting classifier：</span><span style="font-size: 18pt;"><span style="background-color: #fff199;"><b>少数服从多数原则</b></span></span></div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image.png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploading;" width="642px"/><div>这种投票分类器得出的结果经常会比集成中最好的一个分类器结果更好。事实上，<b>即使每一个分类器都是一个弱学习器（意味着它们也就比瞎猜好点），集成后仍然是一个强学习器（高准确率），只要有足够数量的弱学习者，他们就足够多样化。</b></div><div>这怎么可能？接下来的分析将帮助你解决这个疑问。假设你有一个有偏差的硬币，他有<span style="background-color: #fff199;"> 51% 的几率为正</span>面，49% 的几率为背面。如果你实验 1000 次，你会得到差不多 510 次正面，490 次背面，因此大多数都是正面。如果你用数学计算，<b>你会发现在实验 1000 次后，正面概率为 51% 的人比例为 75%。你实验的次数越多，正面的比例越大（例如你试验了 10000 次，总体比例可能性就会达到 97%）</b>。这是因为大数定律 ：当你一直用硬币实验时，正面的比例会越来越接近 51%。下图展示了始终有偏差的硬币实验。你可以看到当实验次数上升时，正面的概率接近于 51%。最终所有 10 种实验都会收敛到 51%，它们都大于 50%。</div><div>同样的，假设你创建了一个包含 1000 个分类器的集成模型，其中每个分类器的正确率只有 51%（仅比瞎猜好一点点）。如果你用投票去预测类别，你可能得到 75% 的准确率！然而，这仅仅<span style="background-color: #fff199;">在所有的分类器都独立运行的很好、不会发生有相关性的错误的情况下才会这样</span>，<span style="color: #FF0000;">然而每一个分类器都在同一个数据集上训练，导致其很可能会发生这样的错误。他们可能会犯同一种错误，所以也会有很多票投给了错误类别导致集成的准确率下降。</span></div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [1].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="574px"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div><span style="color: #FF0000;"><b>当预测器尽可能地相互独立时，集成方法的工作效果最好</b></span>。获得不同分类器的一种方法是使用非常不同的算法来训练它们。这增加了它们产生非常不同类型的错误的机会，提高了集成的准确性。</div></div><div><span style="font-size: 14pt;">2、Soft Voting classifier：</span><span style="font-size: 18pt;"><span style="background-color: #fff199;"><b>平均</b></span></span></div><div style="text-align:start;">如果所有分类器都能够估计类概率（即，它们具有pre dict_proba 方法），那么您可以告诉<a href="https://so.csdn.net/so/search?q=Scikit-Learn&amp;spm=1001.2101.3001.7020" rev="en_rl_none">Scikit-Learn</a>预测获取具有最高类概率的类，<span style="background-color: #fff199;">在所有单个分类器上进行平均</span>。 这被称为软投票。 它通常比硬性投票取得更高的绩效，因为<b>它给予了高度信任的投票更多的权重，再argmax获取值。</b></div><div>你所需要做的就是用voting=“soft”替换投票=“hard”，并确保所有的分类器都可以估计类的概率。<span style="color: #FF0000;">一般而言，</span><span style="font-size: 14pt;"><span style="color: #FF0000;">Soft Voting更优。</span></span></div><div><br/></div><h3>二、Bagging 和 Pasting</h3><div>获得多种分类器的<span style="background-color: #fff199;">一种方式是使用非常不同的训练算法</span>。 <span style="background-color: #fff199;"><b>另一种方法是对每个预测器使用相同的训练算法，但是训练集在</b></span><span style="color: #FF0000;"><span style="background-color: #fff199;"><b>不同的随机子集上进行训练（促进分类器之间的独立性</b></span></span><span style="background-color: #fff199;"><b>）</b></span>。</div><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>When sampling is performedwith replacement, this method is called bagging(short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.</div></div><ul><li><div><span style="font-size: 14pt;"><span style="color: #FF0000;"><b>当采样是有放回时，这种方法被称为bagging （bootstrap aggregating的缩写）</b></span></span></div></li><li><div style="text-align:start;"><span style="font-size: 14pt;"><span style="color: #FF0000;"><b>当采样没有放回时，被称为 pasting。</b></span></span></div></li></ul><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [2].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>对每个预测器使用相同的训练算法，并在训练集的不同随机子集上对它们进行训练。</div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [3].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [4].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="505px"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>代码训练了一个 <span style="background-color: #fff199;">500 </span>个决策树分类器的集成（n_estimators），每一个都是在数据集上<span style="background-color: #fff199;">有放回采样 100 </span>个训练实例(max_samples)下进行训练（这是 Bagging 的例子，如果你想尝试 Pasting，就设置<span style="background-color: #fff199;">bootstrap=False</span>）。n_jobs参数告诉 sklearn 用于训练和预测所需要 CPU 核的数量。（-1 代表着 sklearn 会使用所有空闲核）</div><div><b>max_samples参数如果设置为整数，那么就是从X中抓取出来的样本个数。</b></div><p style="text-align:start;"><b>如果这个参数设置为float，也就是一个比例</b></p></div><p>note<span style="color: #FF0000;">：如果基础分类器可以估计类别概率（即，如果它具有predict_proba（）方法），则BaggingClassifier将自动执行软投票而不是硬投票</span>，这与Decision Trees分类器的情况是一样的。</p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [5].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>正如您所看到的，<b>集成的预测可能比单一决策树的预测概括得更泛化：集成具有类似的偏差，但方差更小（它在训练集上产生大致相同数量的错误，但决策边界不那么不规则）。</b></div><div><br/></div><div>Bootstrap 在每个预测器被训练的子集中引入了更多的分集，所以<b> Bagging 结束时的偏差比 Pasting 更高，但这也意味着预测因子最终变得不相关，从而减少了集合的方差</b>。总体而言，<span style="color: #FF0000;"><span style="background-color: #fff199;">Bagging 通常会导致更好的模型</span></span>，这就解释了为什么它通常是首选的。</div><div><br/></div><ul><li><div><b>Out-of-Bag Evaluation</b></div></li></ul><div>对于 Bagging 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。Bagging Classifier默认采样。Bagging Classifier<b>默认是有放回的采样m个实例</b> （bootstrap=True），其中m是训练集的大小，这意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 Out-of-Bag 实例 【(oob) instances】。注意对于每一个的分类器它们的 37% 不是相同的。</div><img src="Chapter 7.Ensemble Learning and Random Forest_files/5F9F0B8EC9EA7A00AB2DE67867FCB486.jpg" type="image/jpeg" data-filename="5F9F0B8EC9EA7A00AB2DE67867FCB486.jpg" style="--en-uploadstate:uploaded;"/><p><b>因为在训练中分类器从来没有看到过 oob 实例，所以它可以在这些实例上进行评估，而不需要单独的验证集或交叉验证。你可以拿出每一个分类器的 oob 来评估集成本身。</b></p><p>在 sklearn 中，你可以在<span style="background-color: #fff199;">训练后需要创建一个Bagging Classifier来自动评估时设置oob_score=True来自动评估。</span></p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [6].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>对于每个训练实例 oob 决策函数也可通过<b>oob_decision_function_变量来展示</b>。在这种情况下（<b>当基决策器有predict_proba()时）决策函数会对每个训练实例返回类别概率。</b></div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [7].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div><br/></div><h4 style="text-align:start;">随机贴片与随机子空间(Random Patches and Random Subspaces)</h4><div><span style="background-color: #fff199;">Bagging Classifier也支持采样特征</span>。它被两个超参数<b>max_features和bootstrap_features控制。他们的工作方式和max_samples和bootstrap一样，但这是对于特征采样而不是实例采样</b>。因此，每一个分类器都会被在随机的输入特征内进行训练。</div><ul><li><p>Random Patches method： 对特征和训练实例都采样，<span style="color: #FF0000;">每个预测器将在输入特征的一个随机子集上进行训练</span>。当您处理高维输入（如图像）时，此技术特别有用。</p></li><li><div style="text-align:start;">Random Subspaces method：只对特征采样</div></li></ul><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image.jpg" type="image/jpeg" data-filename="Image.jpg" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [8].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div><span style="color: #FF0000;">抽样特征会导致更多的预测器多样性，用更多的偏差来换取更低的方差。</span></div><div><br/></div><h3 style="text-align:start;">三、随机森林</h3><div>1、简介</div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [9].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>正如我们所讨论的，随机森林是决策树的一种集成，<span style="background-color: #fff199;">通常是通过 bagging 方法</span>（有时是 pasting 方法）进行训练，<span style="background-color: #fff199;">通常用max_samples设置为训练集的大小</span>。与建立一个BaggingClassifier然后把它放入 DecisionTreeClassifier 相反，你可以使用更方便的也是对决策树优化够的<span style="background-color: #fff199;">RandomForestClassifier（对于回归是RandomForestRegressor）</span>。</div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [10].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>Extremely Randomized Trees 极度随机树</div><div>当您在随机森林中生成一棵树时，每个节点只有一个随机的特征子集被考虑用于分裂。通过对每个特征使用随机阈值，而不是搜索最佳可能的阈值（像常规决策树那样），可以使树更加随机。 这种非常随机的森林被简称为极度随机树（简称Extra-Trees）。这种方式更偏向于较低的方差。它还使Extra-Tree比常规的随机森林快得多，因为在每个节点上为每个特征找到最佳的阈值是生成决策树中最耗时的任务之一。</div><div>你可以使用 sklearn 的ExtraTreesClassifier来创建一个 Extra-Tree 分类器。他的 API 跟RandomForestClassifier是相同的，相似的， ExtraTreesRegressor 跟RandomForestRegressor也是相同的 API。</div><div>我们很难去分辨ExtraTreesClassifier和RandomForestClassifier到底哪个更好。通常情况下是通过交叉验证来比较它们（使用网格搜索调整超参数）。</div></div><p><span style="font-size: 14pt;">2、feature importance</span></p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [11].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [12].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div><span style="background-color: #fff199;">随机森林非常方便地快速了解什么特征真正重要，特别是如果您需要执行特征选择。（可以代替PCA进行特征选择）</span></div></div><div><br/></div><h2 style="text-align:start;">四、Boosting</h2><div>Boosting，最初称为假设增强，指的是可以将几个弱学习者组合成强学习者的集成方法。<b>对于大多数的提升方法的思想就是按顺序去训练分类器，每一个都要尝试修正前面的分类</b>。现如今已经有很多的提升方法了，但最著名的就是 Adaboost（适应性提升，是 Adaptive Boosting 的简称） 和 Gradient Boosting（梯度提升）</div><p><span style="font-size: 14pt;">1、Adaboost</span></p><p>使一个新的分类器去修正之前分类结果的方法就是对之前分类结果不对的训练实例多加关注。这导致新的预测因子越来越多地聚焦于这种情况。这是 Adaboost 使用的技术。</p><p>举个例子，去构建一个 Adaboost 分类器，<span style="color: #FF0000;"><b>第一个基分类器（例如一个决策树）被训练然后在训练集上做预测，在误分类训练实例上的权重就增加了。第二个分类机使用更新过的权重然后再一次训练，权重更新，以此类推。</b></span></p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [13].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="542px"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [14].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [15].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="546px"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [16].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [17].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div style="text-align:start;"><span style="color: #FF0000;"><b>为了进行预测，Adaboost 通过分类器权重 αj 简单的计算了所有的分类器和权重。预测类别会是权重投票中主要的类别。</b></span></div><div>一旦所有的分类器都被训练后，除了分类器根据整个训练集上的准确率被赋予的权重外，集成预测就非常像Bagging和Pasting了。</div><p>序列学习技术的一个重要的缺点就是：<span style="background-color: #fff199;">它不能被并行化（只能按步骤</span>），因为每个分类器只能在之前的分类器已经被训练和评价后再进行训练。因此，它不像Bagging和Pasting一样。</p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [18].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div><span style="font-size: 14pt;">2、Gradient Boosting</span></div><div><b>与 Adaboost 一样，梯度提升也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果。</b><span style="background-color: #fff199;">然而，它并不像 Adaboost 那样每一次迭代都更改实例的权重，这个方法是去</span><span style="color: #FF0000;"><span style="background-color: #fff199;">使用新的分类器去拟合前面分类器预测的残差 。</span></span></div><p>让我们通过一个使用决策树当做基分类器的简单的回归例子（回归当然也可以使用梯度提升）。这被叫做梯度提升回归树（GBRT，Gradient Tree Boosting 或者 Gradient Boosted Regression Trees）。首先我们用DecisionTreeRegressor去拟合训练集（例如一个有噪二次训练集）：</p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [19].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [20].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="468px"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [21].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>为了找到树的最优数量，你可以使用early-stop。最简单使用这个技术的方法就是使<span style="background-color: #fff199;">staged_predict()：它在训练的每个阶段（用一棵树，两棵树等，到120课数）返回一个迭代器</span>。加下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成：</div><p><br/></p><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [22].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [23].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="614px"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>GradientBoostingRegressor也支持指定用于训练每棵树的训练实例比例的超参数subsample。例如如果subsample=0.25，那么每个树都会在 25% 随机选择的训练实例上训练。你现在也能猜出来，这也是个高偏差换低方差的作用。它同样也加速了训练。这个技术叫做随机梯度提升。</div></div><p><br/></p><div><span style="font-size: 14pt;">3、XGBoost</span></div><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>值得注意的是，在流行的Python库XGBoost中可以找到一个梯度增强的优化实现，它代表极端梯度增强。这个软件包最初是由陈天启作为分布式（深度）机器学习社区（DMLC）的一部分开发的，它的目标是非常快、可扩展和便携性。事实上，XGBoost通常是ML比赛中获胜作品的一个重要组成部分。</div></div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [24].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><h3>五、Stacking</h3><div>Stacking（stacked generalization 的缩写）。<b>这个算法基于一个简单的想法：不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，我们为什么不训练一个模型来执行这个聚合？</b>下图展示了这样一个<span style="background-color: #fff199;">在新的回归实例上预测的集成。底部三个分类器每一个都有不同的值（3.1，2.7 和 2.9），然后最后一个分类器（叫做 blender 或者 meta learner ）把这三个分类器的结果当做输入然后做出最终决策（3.0）。</span></div><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [25].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="426px"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [26].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="584px"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [27].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploading;" width="642px"/><img src="Chapter 7.Ensemble Learning and Random Forest_files/Image [28].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="538.9500713266762px"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>Q&amp;A</div><div>1.如果您在完全相同的训练数据上训练了五个不同的模型，并且都达到了95％的精度，那么您是否有机会将这些模型结合起来以获得更好的结果？ 如果是这样，怎么样？ 如果没有，为什么？</div><div>答：有，当元学习器之间存在差异时，使用集成学习仍能取得比单个学习器更好的效果。如果模型差别很大（例如，SVM分类器，决策树分类器，Logistic回归分类器等），则效果更好。</div><div><br/></div><div>2.硬投票和软投票分类器有什么区别？</div><div>答：硬投票，每个元学习器的投票权值相等；软投票，软投票分类器计算每个类别的平均估计类别概率，并以最高概率挑选类别。这给予了高置信度的投票更多的权重，往往表现更好，但只有每个分类器能够估计类概率（例如，对于Scikit-Learn中的SVM分类器，您必须设置probability = True），它才有效。</div><div><br/></div><div>3.是否有可能通过分布在多台服务器上来加快对bagging的训练？ pasting , boosting，随机森林, 堆栈泛化呢？</div><div>答：除了boosting，其余方法均可以使用分布式计算来提升训练速度，因为它们每个元学习器的训练都是独立的（堆栈泛化中处于同一层的学习器时独立的），是可以并行的。而 boosting 中，下一个元学习器需要使用上一个学习器训练并evaluation之后，调整权值才可以开始训练，时间上是串行的，因此很难用分布式计算提升速度。</div><div><br/></div><div>4.包外评估有什么好处？</div><div>答：无需额外划分测试集，之间使用包外数据作为测试集。</div><div><br/></div><div>5.什么使Extra-Trees比常规的随机森林更随机？ 这种多余的随机性如何帮助？ 额外的树比常规的随机森林慢吗？</div><div>答：extra-trees 通过对每个特征使用随机阈值，而不是搜索最佳可能的阈值（像常规决策树那样）。</div><div><br/></div><div>6.如果你的AdaBoost对训练数据欠拟合了，你应该调整哪些超参数？</div><div>答：可以尝试增加估算器的数量或减少基本估算器的正则化超参数，也可以尝试稍微提高学习率。</div><div><br/></div><div>7.如果您的gradient boosting 对数据集过拟合了，您是否应该增加或减少学习速率？</div><div>答：降低学习率或者使用early-stop。</div></div><div><br/></div></span>
</div></body></html> 