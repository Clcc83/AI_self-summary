<html>
<head>
  <title>Chapter 4.1 Training Models</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/607057 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="775"/>
<h1>Chapter 4.1 Training Models</h1>

<div><span><h1 style="text-align:center;">Chapter 4.1 Training Models</h1><div>更好地了解事情是如何工作的，可以帮助你快速了解适当的模型，正确的训练算法，以及为你的任务提供一组好的超参数。了解底层的内容还将帮助您调试问题和更有效地执行错误分析。最后，本章中讨论的大部分主题将在理解、构建和训练神经网络方面至关重要.</div><div>第四章主要介绍了线性回归的机器学习方法，这章的特点是代码量比较少，但是数学公式有点多。大家需要一些基本的线性代数和求导数的概念就可以看懂。这章的代码还是比较容易看懂的。</div><h2>一、线性回归（linear regression）：</h2><div><b>（模型经过训练之后可以根据给出的X来预测Y）</b></div><div><span style="font-weight: 600;">：</span>和分类算法不同，回归算法需要给出一个预测值。</div><p style="text-align:start;">简单来说，我们就是要找到一组θ, 一旦找到这组θ， 就用他们来和输入的X做计算就得到预测值y了。</p><img src="Chapter 4.1 Training Models_files/Image.png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>n:为特征数量，故参数有n+1个</div><img src="Chapter 4.1 Training Models_files/KHO9})KU57[B@BG7A0GLCQ9.png" type="image/png" data-filename="KHO9})KU57[B@BG7A0GLCQ9.png" style="--en-uploadstate:uploaded;"/><div><b>在机器学习中，向量通常被表示为列向量</b>，<b>这是具有一列的二维数组</b>，此处西格玛表示为行向量。</div><img src="Chapter 4.1 Training Models_files/微信图片_20230314225721.jpg" type="image/jpeg" data-filename="微信图片_20230314225721.jpg" style="--en-uploadstate:uploaded;" width="563.4767321444106px"/><div><br/></div><div>为了得倒这组θ，需要引入一个概念叫做损失函数（cost function）, 损失函数的意思就是我们用训练集里面的X来预测得到ŷ, 如果这个 ŷ 和真实的y之间的差别越大，我们就说损失的越厉害，ŷ和y的差别越小，我们就说损失越小。所以得到一组最优的θ， 使得ŷ和y的差别最小化，我们的目的就达到了。</div><div><br/></div><p style="text-align:start;"><b>在线性回归里面，我们喜欢采用MSE (Mean Square Error)来做损失函数</b>。把X和θ都向量话以后，我们看到的Cost Function如下：</p><img src="Chapter 4.1 Training Models_files/O1Y%_N{29Y11WY)TO{]35`H.png" type="image/png" data-filename="O1Y%_N{29Y11WY)TO{]35`H.png" style="--en-uploadstate:uploaded;"/><div>总结（一元时）：【1/2为了方便求偏导时消除】</div><img src="Chapter 4.1 Training Models_files/YJ8`NH330QX3(A7Q~8M5)LA.png" type="image/png" data-filename="YJ8`NH330QX3(A7Q~8M5)LA.png" style="--en-uploadstate:uploaded;" width="502px"/><div><b>那么如何才能找到正确的θ，使这个Cost Function有一个最小的值呢？可以有下面几种方式：</b></div><ul><li><p style="text-align:start;"><span style="font-size: 12pt;"><span style="background-color: #fff199;"> 利用线性代数的公式直接计算出结果。</span></span></p></li><li><p style="text-align:start;"><span style="font-size: 12pt;"><span style="background-color: #fff199;">用梯度下降的方法来一步一步调整θ， 逐渐逼近Cost Function的最小值</span></span>。</p></li></ul><div><span style="font-size: 14pt;">1、利用数学公式直接得到答案：</span></div><p style="text-align:start;"><b>The Normal Equation【正规方程】</b>: 最小二乘法-直接用数学公式得到 - 是的，你没有看错，直接使用线性代数的公式可以直接计算出最优的数学模型。公式如下：</p><img src="Chapter 4.1 Training Models_files/R0PKB1Z`K(0[IICRPEQO0_J.png" type="image/png" data-filename="R0PKB1Z`K(0[IICRPEQO0_J.png" style="--en-uploadstate:uploaded;" width="570px"/><img src="Chapter 4.1 Training Models_files/6{VOV_X{7F9V}1FX0$_R%0O.png" type="image/png" data-filename="6{VOV_X{7F9V}1FX0$_R%0O.png" style="--en-uploadstate:uploaded;"/><div>具体案例见jupyter</div><div><br/></div><div>也可直接用LinearRegression，线性回归类基于scipy.linalg.lstsq（）函数（名称代表“最小二乘”），您可以直接调用它。伪逆本身是使用一种称为奇异值分解（SVD）的标准矩阵分解技术来计算的，该技术可以将训练集矩阵X分解为三个矩阵U Σ V的矩阵乘法（见numpy.linalg.svd（））。</div><div><span style="font-weight: bold;">Computational Complexity</span></div><img src="Chapter 4.1 Training Models_files/~L2A8$F{WLTE}HA0{DOSURH.png" type="image/png" data-filename="~L2A8$F{WLTE}HA0{DOSURH.png" style="--en-uploadstate:uploaded;" width="446px"/><div>缺点：大规模数据时运算慢，一旦你训练了你的线性回归模型（使用正常方程或任何其他算法），预测就会非常快：计算复杂度对于你想要进行预测的实例的数量和特征的数量都是线性的。换句话说，对两倍数量的实例（或两倍数量的特性）进行预测将花费大约两倍数量的时间。</div><div><br/></div><div><span style="font-size: 14pt;">2、梯度下降（gradient descent）</span></div><img src="Chapter 4.1 Training Models_files/7C%]0}[[I8LN%VT~M_5POVH.png" type="image/png" data-filename="7C%]0}[[I8LN%VT~M_5POVH.png" style="--en-uploadstate:uploaded;" width="467px"/><div>梯度下降的原理就是先<span style="font-size: 16pt;"><span style="background-color: #fff199;">随机初始化theta</span></span>, 然后计算出Cost Function，然后我们在每一个θ上面沿着Cost Function的<span style="font-size: 16pt;"><span style="background-color: #fff199;">梯度下降的方向迈出一小步</span></span>，这样下一步所在位置的Cost Function就变小了，我们<b>不停重复这个动作，直到Cost Function最小</b>。</div><img src="Chapter 4.1 Training Models_files/7)TW2J9XK2VH]J9A)EP~OX7.png" type="image/png" data-filename="7)TW2J9XK2VH]J9A)EP~OX7.png" style="--en-uploadstate:uploaded;" width="423.75853326683216px"/><div><span style="font-size: 16pt;"><span style="background-color: #fff199;"><span style="font-weight: bold;">（1）Gradient descent algorithm</span></span></span></div><img src="Chapter 4.1 Training Models_files/Image [1].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="478px"/><img src="Chapter 4.1 Training Models_files/2796DB406B897CD0BDD0A649315B94D8.jpg" type="image/jpeg" data-filename="2796DB406B897CD0BDD0A649315B94D8.jpg" style="--en-uploadstate:uploaded;" width="414px"/><img src="Chapter 4.1 Training Models_files/9K5M{R`Q]PT8@M7W{NY`0)I.png" type="image/png" data-filename="9K5M{R`Q]PT8@M7W{NY`0)I.png" style="--en-uploadstate:uploaded;" width="531px"/><div><span style="font-size: 14pt;">（2）学习率（learning rate）<span style="background-color: #fff199;"><b>经验：步长0.01</b></span></span></div><img src="Chapter 4.1 Training Models_files/Image [2].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="521.1988636363636px"/><div>当我们接近一个局部最小值时，梯度下降将自动采取更小的步骤。所以，不需要随着时间的推移而减少α。</div><img src="Chapter 4.1 Training Models_files/Image [3].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="482px"/><div><span style="font-size: 14pt;">（3）凸函数（Convex Function）<span style="background-color: #fff199;"><b>经验：做30次求平均</b></span></span></div><img src="Chapter 4.1 Training Models_files/$GYK0~E(I$XYS0$O_GYFGKK.png" type="image/png" data-filename="$GYK0~E(I$XYS0$O_GYFGKK.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 4.1 Training Models_files/Q@OPN3KAXHSL6]LD{JLMNYJ.png" type="image/png" data-filename="Q@OPN3KAXHSL6]LD{JLMNYJ.png" style="--en-uploadstate:uploaded;" width="410px"/><div>因为上面两个图不是凸函数</div><img src="Chapter 4.1 Training Models_files/`3Z22I)[1K$AEV~62HM}O7I.png" type="image/png" data-filename="`3Z22I)[1K$AEV~62HM}O7I.png" style="--en-uploadstate:uploaded;" width="474px"/><div><br/></div><div>梯度下降也分为：<span style="background-color: #fff199;">带入的样本量m值选取方式不同</span></div><ol><li><div><i><u>Batch Gradient Descent批量梯度下降（每次计算梯度，把</u></i><b><i><u>所有训练集的X都用上，一次既一轮</u></i></b><i><u>）</u></i></div></li><li><div><i><u>Stochastic Gradient Descent 随机梯度下降（每次计算梯度，随机的用一个训练集里面的</u></i><b><i><u>某一个X，len次为一轮</u></i></b><i><u>）</u></i></div></li><li><div><i><u>Min-batch Gradient Descent 小批量梯度下降（每次计算梯度，随机的</u></i><b><i><u>选取一些训练集里面的X，len/m次为一轮</u></i></b><i><u>）</u></i></div></li></ol><div><span style="font-weight: 600;">Batch Gradient Descent</span>批量梯度下降<span style="font-weight: 600;">代码：</span></div><img src="Chapter 4.1 Training Models_files/0438DB65F59031C2F232D5946657BD46.jpg" type="image/jpeg" data-filename="0438DB65F59031C2F232D5946657BD46.jpg" style="--en-uploadstate:uploaded;" width="536.608091244645px"/><img src="Chapter 4.1 Training Models_files/99@[IMBA$]N)IQ50POWW6XF.png" type="image/png" data-filename="99@[IMBA$]N)IQ50POWW6XF.png" style="--en-uploadstate:uploaded;" width="570px"/><img src="Chapter 4.1 Training Models_files/PKQH_B[MV}}6`IE7F%@8S$D.png" type="image/png" data-filename="PKQH_B[MV}}6`IE7F%@8S$D.png" style="--en-uploadstate:uploaded;" width="571.4571981062512px"/><div>要找到良好的学习速度，您可以使用网格搜索（参见第2章）。但是，您可能希望限制迭代的次数，以便网格搜索可以消除需要太长时间才能收敛的模型。您可能想知道如何设置迭代的次数。如果它过低，当算法停止时，你仍然会远离最优解；但如果它太高，您将浪费时间，而模型参数不再改变。一个简单的解决方案是设置大量的迭代，但当梯度向量变得很小时中断算法——也就是说，当它的范数小于一个很小的数字<span style="background-color: #fff199;">ϵ（称为容忍度）</span>时——因为当梯度下降（几乎）达到最小值时发生算法</div><div><br/></div><div><span style="font-weight: 600;">Stochastic Gradient Descent 随机梯度下降：</span></div><img src="Chapter 4.1 Training Models_files/RSAB`N$Z[@H0F3}D$LTGLT7.png" type="image/png" data-filename="RSAB`N$Z[@H0F3}D$LTGLT7.png" style="--en-uploadstate:uploaded;" width="335px"/><div><span style="font-weight: 600;">好处：</span></div><ul><li><div>每次计算梯度只用一个X，计算量非常小。</div></li><li><div>随机跳动，有很大可能可以找到全局最优而不是局部最优。</div></li></ul><p style="text-align:start;"><span style="font-weight: 600;">坏处：</span></p><ul><li><div>在接近最优点的时候反复探底，但是反复跳跃，无法收敛。解决方法是逐渐减小步长（learning rate) 如上面的代码所示。</div></li></ul><div><br/></div><ol><li><div>当代价函数非常<b>不规则</b>时，这实际上可<b>以帮助算法跳出局部最小值</b>。随机梯度下降比批处理梯度下降有更好的机会找到全局最小值。</div></li><li><div>随机性是逃避局部最优的好方法，但也不好，因为它意味着算法永远不能达到最小值。</div></li></ol><div>        •解决这个困境的一个方法是逐步降低学习率。</div><div>        •<span style="background-color: #fff199;"><b>这些步骤开始很大（这有助于快速进展和逃避局部最小值），然后越来越小，允许算法达到全局最小值。这个过程类似于模拟退火</b></span></div><div><span style="background-color: #fff199;">学习计划</span>：</div><div>如果学习速度降低得太快，你可能会陷入当地的最低水平，甚至会被冻结到最低水平的一半。</div><div>如果学习率降低得太慢，你可能会在很长一段时间内跳过最小值，如果你过早地停止训练，最终会得到一个次优的解决方案。</div><img src="Chapter 4.1 Training Models_files/be7d68b29efcbe0fa750349ee6133df.png" type="image/png" data-filename="be7d68b29efcbe0fa750349ee6133df.png" style="--en-uploadstate:uploaded;" width="594px"/><div>按照惯例，我们通过<b>m次迭</b>代；<b>每一轮迭代</b>都被称为一个（<span style="font-weight: bold;"><i>epoch</i></span>）时代。</div><div><span style="background-color: #fff199;">在所有训练数据【总数满足就行，可以有重复选择。如<span style="font-weight: 600;">Stochastic，每次选一个数据，for i in range(m)才叫一轮；而Batch用全部数据，就一次为一轮；而Mini-Batch，for i in range(0, m, minibatch_size)为一轮</span>】上迭代一次叫做一个轮次，即epochs</span></div><div>批处理梯度下降代码在整个训练集中迭代了1000次，而该代码只经过训练集50次，得到了一个很好的解决方案。</div><div>请注意，由于实例是随机选择的，有些实例可能在每个时代选择几次，而其他实例可能根本不选择。</div><div>当使用随机梯度下降时，<span style="background-color: #fff199;"><b>训练实例必须是独立的和同分布的（IID），以确保参数平均被拉向全局最优</b></span>。确保这一点的一种<b>简单方法是在训练期间打乱实例</b><span style="color: #A30000;">（例如，随机选择每个实例，或者在每个时代开始时打乱训练集）</span>。如果您不对实例进行洗牌——例如，如果实例是按标签排序的——那么SGD将首先优化一个标签，然后优化下一个标签，以此类推，它将不会接近全局最小值。</div><div>要使用带有Scikit-Learn的随机GD执行线性回归，您可以使用<b>SGDRegressor</b>类，它默认为优化平方误差成本函数</div><img src="Chapter 4.1 Training Models_files/Image [4].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div><span style="background-color: #fff199;"><b>note:总迭代次数 = 总样本数/batchsize *epoch</b></span></div><div>epoch时期：一个epoch是指将所有训练样本都训练一遍</div><div>batchsize批次大小：是指在一次迭代中样本的个数，也即批梯度下降中用来进行一次参数更新的样本数</div><div>iter：是指完成一个epoch所需的迭代次数</div></div><div><span style="font-weight: 600;">Mini-Batch Gradient Descent （小批量随机梯度下降）</span></div><img src="Chapter 4.1 Training Models_files/7aebf724470c915d387355615b95f2c.png" type="image/png" data-filename="7aebf724470c915d387355615b95f2c.png" style="--en-uploadstate:uploaded;" width="523.1721875482561px"/><div>epoch是指<span style="background-color: #fff199;"><b>随机梯度下降算法（for i in range 里）</b></span><b>的迭代次数</b>，即在训练模型时，随机梯度下降算法会迭代多少次。每次迭代时，算法会随机选择一个样本进行训练，然后更新模型参数。</div><p>而max_iter是指<span style="background-color: #fff199;"><b>随机梯度下降算法</b></span><b>的最大迭代次数（</b><span style="background-color: #fff199;"><b>即epoch能达到的最大值</b></span><b>）</b>，即在训练模型时，随机梯度下降算法会迭代多少次。如果达到了最大迭代次数，算法就会停止迭代，返回当前的theta值。</p><p><br/></p><p>其中所指的随机梯度下降算法指：</p><img src="Chapter 4.1 Training Models_files/Image [5].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="666.4000244140625px"/><div><br/></div><ul><li><div>算法对比</div></li></ul><img src="Chapter 4.1 Training Models_files/Image [6].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>note：经过训练后，几乎没有任何区别：所有这些算法最终都有非常相似的模型，并以完全相同的方式进行预测</div><div><br/></div><div><br/></div><div>note：</div><p style="padding-left:40px;text-align:start;"><span style="background-color: #fff199;">如果你想切换到批量梯度下降算法，可以通过将SGDRegressor函数中的batch_size参数设置为整个训练集的样本数量来实现</span>。这样做可以让算法使用整个训练集来更新模型参数，从而实现批量梯度下降。</p><p style="text-align:start;">具体来说，可以将SGDRegressor的batch_size参数设置为训练集的样本数量，即：</p><div style="padding-left:40px;">from sklearn.linear_model import SGDRegressor </div><div style="padding-left:40px;">model = SGDRegressor<span style="background-color: #fff199;">(batch_size=len(X_train))</span></div><p style="text-align:start;">需要注意的是，使用批量梯度下降算法可能会导致模型收敛速度变慢，因为每次更新都需要使用整个训练集，而且在处理大型数据集时可能会出现内存不足的问题。因此，通常情况下，随机梯度下降算法更加常用，尤其是在处理大型数据集时。</p></span>
</div></body></html> 