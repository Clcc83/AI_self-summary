<html>
<head>
  <title>Chapter 4.3 Training Models</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/607057 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="943"/>
<h1>Chapter 4.3 Training Models</h1>

<div><span><h1 style="text-align:center;">Chapter 4.3 Training Models</h1><h2>三、Logistic Regression（逻辑回归）</h2><div>分类基础概念详见第三章</div><img src="Chapter 4.3 Training Models_files/@N~SV3IPJD_$[B`EI_@LX~9.png" type="image/png" data-filename="@N~SV3IPJD_$[B`EI_@LX~9.png" style="--en-uploadstate:uploaded;"/><div>和上面的LinearRegression不同，Logistic Regression的任务是预测True or False, 他是一种 Binary Classifer。我们的任务是找到一组最优的θ，用这组θ和下面的公式来预测可能性：（p表示的就是可能性）</div><img src="Chapter 4.3 Training Models_files/Image.png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div><b>Sigmoid函数</b></div><img src="Chapter 4.3 Training Models_files/2$)OE544VCJA@1`A@B0_`BX.png" type="image/png" data-filename="2$)OE544VCJA@1`A@B0_`BX.png" style="--en-uploadstate:uploaded;" width="510px"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>note: 分数t通常被称为logit。这个名称来自于logit函数，定义为logit(p)=log（p/（1-p）），是逻辑函数的倒数。事实上，如果你计算估计概率p的logit，你会发现结果是t。logit也被称为对数概率，因为它是正类的估计概率和负类的估计概率之比的对数,即X转置乘θ。</div></div><div>输出一个介于0到1之间的数字。</div><img src="https://pic1.zhimg.com/80/v2-b84488796a3912f82da058a48d87e03c_720w.webp"></img><div>一旦Logistic回归模型估计了一个实例x属于正类的概率= hθ (x)，它就可以使它很容易地预测y。连续变量转换为离散二元变量</div><img src="Chapter 4.3 Training Models_files/WRK@~)2L}MPA7S}IH7MJN0W.png" type="image/png" data-filename="WRK@~)2L}MPA7S}IH7MJN0W.png" style="--en-uploadstate:uploaded;" width="511px"/><div><br/></div><h3>Training and Cost Function</h3><img src="Chapter 4.3 Training Models_files/FPX5)$H71YR0`3[WN6)Z8}M.png" type="image/png" data-filename="FPX5)$H71YR0`3[WN6)Z8}M.png" style="--en-uploadstate:uploaded;" width="510px"/><img src="Chapter 4.3 Training Models_files/6PTR5D%KPQ8$SAMNUG(NK@A.png" type="image/png" data-filename="6PTR5D%KPQ8$SAMNUG(NK@A.png" style="--en-uploadstate:uploaded;" width="574px"/><div>让我们回忆一下cost function的含义，当预测值和真实值之间的差距越大，cost就越大，预测值和真实值之间的差越小，Cost Function就越小。为了方便大家更加容易理解，我画了一下</div><p style="text-align:start;"><i>-log(p) </i>和 <i>-log(1-p)</i>的曲线供大家参考</p><img src="https://pic2.zhimg.com/80/v2-2263ff7930b9caec4e5ea8696cfe5f35_720w.webp" width="330"></img><img src="Chapter 4.3 Training Models_files/4C}%D9UZE$LI7UI85H50[DW.png" type="image/png" data-filename="4C}%D9UZE$LI7UI85H50[DW.png" style="--en-uploadstate:uploaded;" width="526px"/><div>坏消息是，<b>没有已知的封闭形式的方程</b>来计算θ的值来最小化这个代价函数（没有等同于正规方程）。好消息是，这个<span style="background-color: #fff199;">代价函数是凸的，所以梯度下降（或任何其他优化算法）可以保证找到全局最小值</span>（如果学习率不是太大，而且你等待的时间足够长）。</div><img src="Chapter 4.3 Training Models_files/Image [1].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="474px"/><div><span style="font-size: 18pt;"><span style="background-color: #fff199;"><b>梯度的计算</b></span></span></div><img src="Chapter 4.3 Training Models_files/Image [2].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="588.2034191896439px"/><img src="Chapter 4.3 Training Models_files/CX@CBGV3F7ZDOJC6B@LBOOD.png" type="image/png" data-filename="CX@CBGV3F7ZDOJC6B@LBOOD.png" style="--en-uploadstate:uploaded;" width="526px"/><div>对于每个实例，它计算预测误差，并将其乘以第j个特征值，然后它计算所有训练实例的平均值。一旦您有了包含所有偏导数的梯度向量，您就可以在批处理梯度下降算法中使用它。就是这样：你现在知道如何训练一个逻辑回归模型了。对于随机GD，您将一次取一个实例，而对于小批量GD，您将一次使用一个小批量。（极其类似线性回归）</div><img src="Chapter 4.3 Training Models_files/KV{WZ`VO@UUPY){YAZ9KU9A.png" type="image/png" data-filename="KV{WZ`VO@UUPY){YAZ9KU9A.png" style="--en-uploadstate:uploaded;" width="554px"/><div><br/></div><h3>Softmax Regression</h3><div>Logistic Regression不但可以做Binary Classifier，还可以做<span style="color: #FF0000;">多分类器</span>，但是这里的方式和我们在第三章的OvO和OvR的方式不同（<span style="background-color: #fff199;">而不需要训练和组合多个二进制分类器</span>）。</div><div>Softmax Regression的思想是通过logit （<span style="background-color: #fff199;">e的指数化）来得到不同分类相应的possibility或者分类标签。</span></div><p style="text-align:start;">Softmax Regression的Possibility预测公式如下：</p><img src="Chapter 4.3 Training Models_files/Image.webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;"/><img src="Chapter 4.3 Training Models_files/Image [3].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="559px"/><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>note：The Softmax Regression classifier predicts only one class at a time (i.e., <span style="color: #FF0000;"><b>it is</b></span></div><div><span style="color: #FF0000;"><b>multiclass, not multioutput</b></span>), so it should be used only with mutually exclusive</div><div>classes, such as different types of plants. You cannot use it to recognize multiple</div><div>people in one picture.</div></div><img src="Chapter 4.3 Training Models_files/}N9FJ[GAORREY]7_E}ASE_P.png" type="image/png" data-filename="}N9FJ[GAORREY]7_E}ASE_P.png" style="--en-uploadstate:uploaded;"/><p>注意，每个类都有自己的专用参数向量θ (k)。所有这些向量通常都以行的形式存储在一个参数矩阵Θ中。</p><img src="Chapter 4.3 Training Models_files/Image [4].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>因为<span style="background-color: #fff199;">当模型估计一个目标类的低概率时，它会惩罚模型</span>。交叉熵经常被用来衡量一组估计的类概率与目标类的匹配程度。</div><img src="Chapter 4.3 Training Models_files/NT($@C22@PIQ[QW)6ZA3461.png" type="image/png" data-filename="NT($@C22@PIQ[QW)6ZA3461.png" style="--en-uploadstate:uploaded;" width="543px"/><ul><li><div>多分类问题时：Scikit-Learn’sLogisticRegressionuses one-versus-the-rest by <b>default when you train it on more than two classes</b>,<span style="background-color: #fff199;"> but you can set the multi_class</span> <span style="background-color: #fff199;"><b>hyperparameter to&quot;multinomial&quot;to switch it to Softmax Regression. Y，</b></span></div></li><li><div><span style="background-color: #fff199;"><b>您还必须指定一个支持Softmax回归的求解器，例如“lbfgs”求解器（有关更多详细信息，请参见Scikit-Learn的文档）。</b></span></div></li><li><div><span style="background-color: #fff199;"><b>它还在默认情况下应用了ℓ2正则化，您可以使用超参数C（惩罚系数的倒数）来控制它</b></span></div></li></ul><div><b>例子：</b></div><div>X = iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</div><div> y = iris[&quot;target&quot;]   </div><div>softmax_reg = LogisticRegression(<span style="background-color: #fff199;">multi_class=&quot;multinomial&quot;,</span>                                    <span style="background-color: #fff199;">solver=&quot;lbfgs&quot;, C=10, random_state=42)</span>  softmax_reg.fit(X, y)  softmax_reg.predict([[5,2]])  </div><div>OUTPUT&gt;&gt; array([2])   </div><div>#分类为第2种花，就是Iris Virginica     </div><div>myarray = softmax_reg.predict_proba([[5, 2]])   #这里我做了一个小处理，直接打印myarray所用的科学计算法有点看不清结果，用  # np.printoptions 函数可以美化打印结果   with np.printoptions(precision=3, suppress=True):      </div><div>             print(myarray)    </div><div>OUTPUT&gt;&gt; [[0.    0.057 0.943]] </div><img src="Chapter 4.3 Training Models_files/6G][B`@HE$[[2YNJKYQ`TE2.png" type="image/png" data-filename="6G][B`@HE$[[2YNJKYQ`TE2.png" style="--en-uploadstate:uploaded;" width="578px"/><div><br/></div></span>
</div></body></html> 