<html>
<head>
  <title>Chapter 6. Decision Trees</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/607057 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="985"/>
<h1>Chapter 6. Decision Trees</h1>

<div><span><h1 style="text-align:center;">Chapter 6. <span style="font-weight: bold;">Decision Trees</span></h1><div><br/></div><div><span style="font-weight: 600;">1、决策树模型的基本概念</span></div><p style="text-align:start;">决策树基于树的结构进行决策，从根节点开始，沿着划分属性进行分支，直到叶节点：</p><ul><li><div><span style="font-weight: 600;">“内部结点”</span>：有根结点和中间结点，某个属性上的测试（test），这里的test是针对属性进行判判断。（若为连续变量则拆分为多个范围，通常拆成二分类）</div></li><li><div><span style="font-weight: 600;">分支</span>：该测试的可能结果，属性有多少个取值，就有多少个分支</div></li><li><div><span style="font-weight: 600;">“叶节点”</span>：预测结果</div></li></ul><img src="Chapter 6. Decision Trees_files/Image.png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>note: 决策树的许多特性之一是，它们只需要很少的数据准备。事实上，它们根本不需要归一化或中心化，容许缺失值。</div><div><span style="font-weight: 600;">2、 决策树算法的历史</span></div><ul><li><div>第一个决策算法（E.B.Hunt）：CLS</div></li><li><div>使决策树受到关注，成为机器学习主流技术的算法（J.R.Quinlan）：ID3</div></li><li><div>最常用的决策树算法（J.R.Quinlan）：C4.5</div></li><li><div><b>既可以<span style="font-weight: 600;">分类</span>可以用于<span style="font-weight: 600;">回归</span>任务的决策树算法</b>：CART（Classfication and regression tree），<span style="color: #FF0000;">从统计建模的角度出发考虑问题（在决定分裂时使用基尼多样性指数作为杂质的衡量标准），前面都是用过信息论角度去考虑</span></div></li><li><div>基于决策树的最强大算法之一：Random Forest</div></li></ul><h3><span style="background-color: #fff199;"><b>3、决策树的关键——如何找合适的“划分属性”</b></span></h3><div><span style="font-size: 14pt;"><span style="font-weight: 600;">3.1  ID3-信息增益（information gain）</span></span></div><p style="text-align:start;">这里的信息是从信息论的信息，信息论里有一个非常重要的概念——<span style="font-weight: 600;">信息熵</span>，其中这个“熵”（entropy）是指对复杂系统的刻</p><p style="text-align:start;">画，可以理解为系统由不稳定态到稳定态所需要丢失的部分，信息熵可以理解为信息由不干净到干净所需要丢失的部分。<b>（每个结点的）信息熵满足公式</b>：</p><img height="151" src="https://pic2.zhimg.com/80/v2-738252aa284e221095bbc80fd17e98b1_720w.webp" width="463"></img><div><span style="background-color: #fff199;"><b>y是类别的数量</b></span>，pk指划分到类别k的概率，整个数据划分干净时信息熵为0，信息熵最大为log2。</div><img src="Chapter 6. Decision Trees_files/562c11dfa9ec8a131ccbc237fc03918fa1ecc010.jpg" type="image/jpeg" data-filename="562c11dfa9ec8a131ccbc237fc03918fa1ecc010.jpg" style="--en-uploadstate:uploaded;" width="430px"/><img src="Chapter 6. Decision Trees_files/JF}3[)I{Y17_QU24CNN3IQS.png" type="image/png" data-filename="JF}3[)I{Y17_QU24CNN3IQS.png" style="--en-uploadstate:uploaded;" width="450px"/><img src="Chapter 6. Decision Trees_files/Image [1].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploading;" width="638"/><div><span style="background-color: #fff199;"><b>V为结点数，即分类后的类别数。</b></span></div><img src="Chapter 6. Decision Trees_files/Image [2].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="610px"/><img src="Chapter 6. Decision Trees_files/Image [3].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="542px"/><div>右图：极端情况，每个样本都分为一个结点。</div><img src="Chapter 6. Decision Trees_files/9%9567TK22P%C8R$%1(T(AS.png" type="image/png" data-filename="9%9567TK22P%C8R$%1(T(AS.png" style="--en-uploadstate:uploaded;" width="560px"/><div><span style="color: #FF0000;"><b>显然，信息增益倾向于选择值较多的特征。</b></span></div><div><br/></div><div><br/></div><div><span style="font-size: 14pt;"><span style="background-color: #fff199;"><b>When to Stop the Tree</b></span></span></div><ul><li><div>叶节点的样本属于同一类（The impurity of all nodes is zero）</div></li><li><p>叶节点样本个数少于阈值（Node size is too small）</p></li><li><p>拆分后的信息增益不够高（No split achieves a significant gain in purity）</p></li></ul><img src="Chapter 6. Decision Trees_files/}1L3I5VZ5[$_ECR{N@K6ZRJ.png" type="image/png" data-filename="}1L3I5VZ5[$_ECR{N@K6ZRJ.png" style="--en-uploadstate:uploaded;"/><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 14pt;">3.2 C4.5-增益率（gain ratio）</span></div><p style="text-align:start;">不像信息增益偏好取值数目多的属性，增益率偏好属性取值那一项获得的数量多的属性，<span style="background-color: #fff199;">除以IV可以看做是一个归一化（标准化）过程</span>，<span style="color: #FF0000;">这里，属性a的可能取值数目越多，IV（a）值越大，信息增益除以IV后值越小，抵消了分支多的好处</span>。但是这样一来，<b>增益率会偏好分支少的属性</b>，<span style="background-color: #fff199;">C4.5采用两步对此进行了折中改进：</span></p><ul><li><div>先对所有属性的信息增益进行从高到低排序，从候选划分属性中选出信息增益高于平均水平的；</div></li><li><div>在从中选出增益率高的；</div></li></ul><img src="Chapter 6. Decision Trees_files/Image.jpg" type="image/jpeg" data-filename="Image.jpg" style="--en-uploadstate:uploaded;"/><div><br/></div><div><span style="font-size: 14pt;"><b>4.3 CART-基尼系数</b></span></div><img src="Chapter 6. Decision Trees_files/Image [4].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>计算例子：</div><img src="Chapter 6. Decision Trees_files/~SD4ZWYV$9~U7[M[I0CDPYB.png" type="image/png" data-filename="~SD4ZWYV$9~U7[M[I0CDPYB.png" style="--en-uploadstate:uploaded;"/><div>对分类后个结点加权：</div><img src="Chapter 6. Decision Trees_files/0QQXG6K74}W7M$6ZLC%%WG0.png" type="image/png" data-filename="0QQXG6K74}W7M$6ZLC%%WG0.png" style="--en-uploadstate:uploaded;" width="531px"/><img src="Chapter 6. Decision Trees_files/T{A7LE6CWXNR{HT3ZJ~33WT.png" type="image/png" data-filename="T{A7LE6CWXNR{HT3ZJ~33WT.png" style="--en-uploadstate:uploaded;"/><div><span style="font-size: 14pt;"><span style="background-color: #fff199;"><b>选基尼系数最小的分类标准</b></span></span></div><div><br/></div><ul><li><div><span style="font-size: 18pt;">Gini Impurity or Entropy?</span></div></li></ul><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div>Most of the time it does not make a big difference: they lead to similar trees. </div><div>Gini impurity is slightly faster to compute, so it is a good default.</div><div>However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.</div></div><div><br/></div><h3><span style="background-color: #fff199;"><b>4、剪枝（pruning）</b></span></h3><p style="text-align:start;">在上面我们提到了信息增益、增益率、基尼系数三个不同的划分属性，有人会疑惑，不同的划分原则是否会对结果产生影响，<b>研究表明划分选择的各种准则虽然对决策树的尺寸有较大的影响，但对泛化性能的影响有限</b>。相比而言，<span style="background-color: #fff199;">剪枝方法和程度对决策树泛化性能的影响更为显著</span>。决策树从上到下划分实际上完成的是从全部到局部的划分，分到局部时可能会受到噪音的影响，容易产生不必要的分枝而过拟合，<span style="background-color: #fff199;">剪枝是决策树对付“过拟合”的主要手段</span>。</p><p style="text-align:start;">5.1 剪枝的两种思路：</p><ul><li><div>预剪枝（pre-pruning）：提前终止某些分支的生长</div></li><li><div>后剪枝（post-pruning）：生成一颗完整树，再回头从下往上“修剪”</div></li></ul><div style="--en-blockquote:true;box-sizing: border-box; padding-left: 19px; padding-top: 6px; padding-bottom: 6px; border-left: 3px solid #b4c0cc; background-position: initial initial; background-repeat: initial initial; margin-top: 6px"><div><span style="color: #FF0000;">决策树分类器类有一些其他参数同样限制决策树的形状：</span></div><div><span style="color: #FF0000;">min_samples_split（节点必须分割的最小样本数量），</span></div><div><span style="color: #FF0000;">min_samples_leaf（叶节点的最小样本数量），</span></div><div><span style="color: #FF0000;">min_weight_fraction_leaf（与min_samples_leaf相同，但表示为加权实例总数的一部分），max_leaf_nodes（叶节点的最大数量），</span></div><div><span style="color: #FF0000;">max_features（每个节点分裂的特性的最大数量）。</span></div><div><span style="color: #FF0000;">增加min_*超参数或减少max_*超参数将使模型规则化。</span></div></div><div><br/></div><div>5.2 剪枝评估</div><p style="text-align:start;">剪枝即剪去不必要的、不应该得到的分支，剪枝的过程需要<span style="background-color: #fff199;">采用模型评估的方法去评估剪枝前后的优劣</span></p><img src="Chapter 6. Decision Trees_files/Image.webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="472px"/><div>西瓜数据集</div><ul><li><div>预剪枝示例：</div></li></ul><p style="text-align:start;"><span style="background-color: #fff199;"><b>划分后精度变低了，则不划分进行剪枝，划分后精度没变化，遵循奥卡姆剃刀原则，不划分</b></span></p><img src="Chapter 6. Decision Trees_files/Image [1].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="450px"/><ul><li><div>后剪枝示例：</div></li></ul><p style="text-align:start;"><span style="background-color: #fff199;"><b>由下到上，每一个结点都要考察是否剪，如果剪前剪后没有变化则不剪</b></span>。</p><img src="Chapter 6. Decision Trees_files/Image [2].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="436px"/><div>考察6号结点</div><img src="Chapter 6. Decision Trees_files/Image [3].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="490px"/><div>考察5号结点</div><img src="Chapter 6. Decision Trees_files/Image [4].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="490px"/><div>考察2号结点</div><img height="310" src="https://pic4.zhimg.com/80/v2-6698c9430002cbd547b5ca2ab5c4e493_720w.webp" width="442"></img><div>考察3和1号结点</div><img height="323" src="https://pic3.zhimg.com/80/v2-31d62f0789e76d2ae027277a417ffe1e_720w.webp" width="394"></img><div>后剪枝完成</div><p style="text-align:start;"><span style="font-size: 14pt;">4.3 预剪枝VS后剪枝</span></p><p style="text-align:start;">（1）时间开销</p><ul><li><div><b>预剪枝：测试时间开销降低，训练时间开销降低</b></div></li><li><div>后剪枝：测试时间开销降低，训练时间开销增加</div></li></ul><p style="text-align:start;">（2）过/欠拟合风险：</p><ul><li><div>预剪枝：过拟合风险降低，欠拟合风险增加</div></li><li><div><b>后剪枝：过拟合风险降低，欠拟合风险基本不变</b></div></li></ul><p style="text-align:start;">（3）<b>泛化性能：后剪枝通常优于预剪枝</b></p><p style="text-align:start;"><br/></p><div style="text-align:start;"><br/></div><div style="text-align:start;"><span style="font-weight: 600;">5. C4.5算法代表性处理机制</span></div><p style="text-align:start;"><span style="font-weight: 600;">5.1 连续属性离散化</span></p><p style="text-align:start;">从离散建模的技术来处理离散连续属性，需要引入一个机制来完成离散化，决策树是在C4.5算法中实现了这一点。注意，在数据处理之前需要对所有的属性进行规范化（标准化），把取值规范到同一范围内，通常变到（0,1）范围内。</p><p style="text-align:start;">常<span style="background-color: #fff199;">见的连续属性离散化做法——二分法（bi-partition）</span></p><ul><li><div>n个不同的值（属性）存在n-1个区间划分</div></li><li><div>将它们视为n-1个离散属性值处理，也可以选择区间里的<span style="background-color: #fff199;">中值作为划分</span>，可以有多个划分点</div></li></ul><p style="text-align:start;">如何评估哪些划分点好？</p><ul><li><div>经过离散化后，连续属性变成离散属性，可以采用信息增益、增益率或者基尼系数去判断选择何种划分点</div></li></ul><img height="195" src="https://pic2.zhimg.com/80/v2-47a066961e3b529949314d0712dbafa5_720w.webp" width="286"></img><p style="text-align:start;"><span style="font-weight: 600;">5.2 缺失值（missing）的基本处理思路</span></p><p style="text-align:start;">直接扔掉缺失值是常见做法，但如果缺失值数量很多的话，每个属性都或多或少有缺失值的话可能浪费很多数据，使用带缺失值的样例，需要解决：</p><p style="text-align:start;"><span style="font-weight: 600;">5.21 如何进行划分属性选择？</span></p><ul><li><div><span style="color: #FF0000;"><span style="font-weight: 600;">在划分阶段：判断划分属性时，只用没有（该分类属性的）缺失值的样本进行判断，这样一来，每一个属性的判断都使用了大部分样本。</span></span>如下图“样本集”，对“色泽”属性进行判断，不考虑缺失样本1,5，13；分别求出“乌黑”、“青绿”、“浅白”三个颜色属性的信息增益，注意要乘每个颜色在非缺失样本中所占比例；最后求“色泽”的信息增益，即每一个颜色的信息增益之和乘以无缺失样本的比例。</div></li></ul><img src="Chapter 6. Decision Trees_files/Image [5].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="438px"/><div>样本集</div><img src="Chapter 6. Decision Trees_files/Image [6].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="507px"/><div>乌黑”、“青绿”、“浅白”三个颜色属性的信息增益</div><img height="148" src="https://pic2.zhimg.com/80/v2-d85c6c61a69527cee585d0856b001c35_720w.webp" width="607"></img><div>“色泽”的信息增益</div><p style="text-align:start;"><span style="font-weight: 600;">6.22给定划分属性，若样本在该属性该怎么做？</span></p><ul><li><div><span style="color: #FF0000;"><b>划分完毕后，向下一结点传递属性：没有缺失值的样本属性，权重为1，对缺失样本我们会在赋权的基础上把它分到所有属性中，对权重进行划分，表示的是缺失样本在后续节点中有可能的分类比重</b></span>。如下图，对于“纹理”这个属性，有“清晰”、“稍糊”、“模糊”三种类别，2个缺失值，其中非缺失值以1的比重向下传递。<span style="background-color: #FFF199;">样本8和10作为缺失值以不同比重分配到三个类别中向下传递，分别是清晰7/15，稍糊5/15，模糊3/15。</span></div></li></ul><img src="Chapter 6. Decision Trees_files/Image [7].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;" width="510px"/><img src="Chapter 6. Decision Trees_files/Image [5].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div><br/></div><div style="text-align:start;"><span style="font-size: 18pt;">6、Regression trees</span></div><img src="Chapter 6. Decision Trees_files/Image [6].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>CART算法的工作方式与之前基本相同，<b>除了它现在不是试图以最小化杂质的方式分割训练集，而是试图以最小化MSE的方式分割训练集。</b></div><img src="Chapter 6. Decision Trees_files/Image [7].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div>请注意，<span style="background-color: #fff199;">每个区域的预测值总是该区域中实例的平均目标值</span>。该算法以一种使大多数训练实例尽可能接近该预测值的方式来分割每个区域。</div><img src="Chapter 6. Decision Trees_files/Image [8].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploading;" width="642px"/><img src="Chapter 6. Decision Trees_files/Image [9].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><img src="Chapter 6. Decision Trees_files/Image [10].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="546px"/><div>与分类任务一样，<b>决策树在处理回归任务时也容易发生过拟合</b>。如果没有任何正则化（即，使用默认的超参数），你就可以得到左边的预测</div><div><span style="font-size: 20pt;"><span style="color: #FF0000;"><b>致命缺陷</b></span></span></div><img src="Chapter 6. Decision Trees_files/Image [11].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploading;" width="642px"/><div><br/></div><div><span style="font-weight: 600;">7. 单变量决策树VS多变量决策树</span></div><p style="text-align:start;"><span style="font-weight: 600;">7.1单变量决策树</span></p><p style="text-align:start;">（在每个非叶结点）每次只针对一个属性进行划分，其他属性保持不变，会出现“轴平行”平面，每个空间的划分区域都对应着一个叶节点（分类）。</p><img height="381" src="https://pic4.zhimg.com/80/v2-e015348d594dd776373df44e650940cb_720w.webp" width="339"></img><div>分类树图（每次只对一个属性进行判断）</div><img height="362" src="https://pic1.zhimg.com/80/v2-d83203fd996e1032af9d77ccc675cfac_720w.webp" width="473"></img><div>轴平行平面</div><p style="text-align:start;"><span style="font-weight: 600;">7.2 多变量决策树</span></p><p style="text-align:start;">当学习任务多对应的分类边界很复杂时，需要非常多段划分能得到较好的近似，于是想用非轴平行的直线进行划分。<span style="font-weight: 600;">多变量决策树：每个非叶结点不仅考虑一个属性，考虑多个属性的组合。</span>如“斜决策树”（oblique decision tree）不是为每一个非叶结点寻找最优划分属性，而是建立一个线性分类器，可以同时看多个变量。</p><img height="202" src="https://pic2.zhimg.com/80/v2-049bd981635dc8a02b939364a5a28641_720w.webp" width="477"></img><div>通过多属性的线性组合进行划分</div><img src="Chapter 6. Decision Trees_files/Image [8].webp" type="image/webp" data-filename="Image.webp" style="--en-uploadstate:uploaded;"/><div>非轴平行平面</div><p style="text-align:start;"><span style="font-weight: 600;">7.3混合决策树</span></p><p style="text-align:start;">多变量决策树相当于是对单变量决策树的拓展，每次划分可以看多个属性，多个属性可以进行线性组合，也可以进行更复杂的模型，甚至神经网络或其他非线性模型。</p></span>
</div></body></html> 