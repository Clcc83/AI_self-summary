<html>
<head>
  <title>Chapter 10. Introduction to Artificial Neural Networks with Keras</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/607057 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="845"/>
<h1>Chapter 10. Introduction to Artificial Neural Networks with Keras</h1>

<div><span><h1 style="text-align:center;">Chapter 10. Introduction to Artificial Neural Networks with Keras</h1><ul><li><div><span style="font-weight: bold;"><i>artificial neural networks</i></span>(ANNs): an ANN is a Machine Learning model <b>inspired by the networks of biological neurons found in our brains.</b></div></li></ul><div>人工神经网络（ANNs）：人工神经网络是一种机器学习模型，其灵感来自于我们大脑中发现的生物神经元络。</div><div><b>ANNs are at the very core of Deep Learning.</b></div><h1>一、introduces artificial neural networks</h1><img src="Chapter 10. Introduction to Artificial Neural_files/H}OZ]G@12`B33Y2X[U`Z1D6.png" type="image/png" data-filename="H}OZ]G@12`B33Y2X[U`Z1D6.png" style="--en-uploadstate:uploaded;" width="490px"/><div>生物神经元产生被称为动作电位（或简称信号）的短电脉冲，这些脉冲沿着轴突移动，使突触释放被称为神经递质的化学信号。当一个神经元在几毫秒内接收到足够数量的这些神经递质时，它就会触发自己的电脉冲（实际上，它依赖于神经递质，因为其中一些会抑制神经元的放电）。</div><div>高度复杂的计算可以通过一个相当简单的神经元网络来完成，就像一个复杂的蚁丘可以从简单的蚂蚁的共同努力中出现一样。生物神经网络（BNNs）5的结构仍然是积极研究的主题，</div><h3>Logical Computations with Neurons</h3><div>人工神经元（<span style="font-weight: bold;"><i>artificial neuron</i></span>）：它有一<b>个或多个二进制（开/关）输入和一个二进制输出</b>。当人工神经元的<span style="background-color: #fff199;">输入超过一定数量时，人工神经元激活其输出。</span></div><div>假设一个神经元在<b>至少有两个输入被激活时被激活</b>。</div><img src="Chapter 10. Introduction to Artificial Neural_files/Image.png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="576px"/><div>1.身份函数：如果神经元A被激活，那么神经元C也被激活。依次类推</div><h3>1、The Perceptron（感知机）</h3><div>The<span style="font-weight: bold;"><i>Perceptron </i></span>is one of the simplest ANN architectures</div><div>It is based on a slightly different artificial neuron called a<span style="font-weight: bold;"><i>threshold logic unit </i></span>(TLU), or sometimes a<span style="font-weight: bold;"><i>linear threshold unit </i></span>(LTU).</div><div>它基于一个略有不同的人工神经元，称为<span style="background-color: #fff199;"><b>阈值逻辑单元</b></span>（TLU）【见下图】，有时也称为线性阈值单元（LTU）。</div><div>输入和输出都是数字（而不是二进制的开/关值），并且每个输入连接都与一个权重相关联。</div><img src="Chapter 10. Introduction to Artificial Neural_files/%[5L6MWE9U6}GK[E3%0JJ$M.png" type="image/png" data-filename="%[5L6MWE9U6}GK[E3%0JJ$M.png" style="--en-uploadstate:uploaded;" width="532px"/><img src="Chapter 10. Introduction to Artificial Neural_files/VYT1}T@VYVA}Z1KWVI_RSOP.png" type="image/png" data-filename="VYT1}T@VYVA}Z1KWVI_RSOP.png" style="--en-uploadstate:uploaded;" width="534px"/><div><span style="font-size: 16pt;"><span style="background-color: #fff199;"><b>一个感知器仅仅由一层TLU组成</b></span></span>，<span style="font-size: 16pt;"><span style="background-color: #fff199;">每个TLU连接到所有的输入</span>。</span></div><div>当<b>一层中的所有神经元都连接到前一层中的每个神经元</b>（eg.它的输入神经元）时，该层被称为完全连接层（<span style="font-weight: bold;"><i>fully connected layer</i></span>），或致密层。</div><div>感知器的输入<b>被输入给特殊的通道神经元</b>，称为输入神经元；<span style="background-color: #fff199;">它们直接输出它们所输入的</span>。所有的输入神经元都构成了输入层。</div><div>此外，通常会添加一个额外的偏置特征（x0 = 1）：它通常使用一种被称为偏置神经元（bias neuron）的特殊类型的神经元来表示，它一直输出1。</div><div>下图为两个输入、三个输出</div><img src="Chapter 10. Introduction to Artificial Neural_files/Image [1].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="520.7123287671233px"/><img src="Chapter 10. Introduction to Artificial Neural_files/8Q_9ID3U)RTJ5KW_$5{RUEH.png" type="image/png" data-filename="8Q_9ID3U)RTJ5KW_$5{RUEH.png" style="--en-uploadstate:uploaded;" width="478px"/><img src="Chapter 10. Introduction to Artificial Neural_files/12PSJRA4XVC2BMAS1_S}14J.png" type="image/png" data-filename="12PSJRA4XVC2BMAS1_S}14J.png" style="--en-uploadstate:uploaded;"/><ul><li><div>和往常一样，<b>X表示输入特征的矩阵</b>。<span style="color: #FF0000;">它的每个实例有一行，每个特性有一列</span>。</div></li><li><div>权值矩阵W包含了<b>除了来自偏置神经元的连接权值外的所有连接权值</b>。<span style="color: #FF0000;">该层中每个输入神经元有一行，每个人工神经元有一列。</span></div></li><li><div>偏置向量b包含<b>偏置神经元与人工神经元之间的所有连接权值</b>。<span style="background-color: #fff199;">每个人工神经元有一个偏差项</span>（可能不同）。</div></li><li><div>这个函数被称为<b>激活函数</b>：当人工神经元是TLU时，它是一个阶跃函数（但我们稍后将讨论其他激活函数）。</div></li></ul><div><br/></div><div>感知器使用该规则的一个变体进行训练，该规则考虑到网络在做出预测时所犯的错误；感知器学习规则加强了有助于减少错误的联系。更具体地说，<span style="color: #FF0000;"><b>感知器一次被输入一个训练实例，并对每个实例做出预测。对于每一个产生错误预测的输出神经元，它会加强输入中有助于正确预测的连接权值</b></span>。</div><div>该规则如等式所示。</div><img src="Chapter 10. Introduction to Artificial Neural_files/KF{ZVG67CMX$8TC%HJR27FY.png" type="image/png" data-filename="KF{ZVG67CMX$8TC%HJR27FY.png" style="--en-uploadstate:uploaded;"/><div>wij是第i个输入神经元与第j个输出神经元之间的连接权值。</div><div>xi是当前训练实例的第i个输入值。</div><div>yj hat是当前训练实例的第j个输出神经元的<b>实际输出（应该没通过激活函数前）</b>。</div><div>yj是当前训练实例的第j个输出神经元的<b>目标输出</b>。</div><div>η是学习速率。</div><div>感知器学习算法非常类似于随机梯度下降。事实上，<b>Scikit-Learn的感知器类相当于使用具有以下超参数的SGD分类器</b></div><div><b>缺陷：只适用于线性分类问题的Rosenblatt感知机无法对异或问题进行分类。然而非线性问题是普遍存在的</b></div><h2>2、The Multilayer Perceptron and Backpropagation</h2><div><span style="font-size: 14pt;"><b>（1）</b></span>每个输出神经元的决策边界都是线性的，因此感知器无法学习复杂的模式（就像逻辑回归分类器一样）。</div><div>通过叠加多个感知机，可以消除感知机的一些局限性。由此产生的人工神经网络被称为多层感知器（MLP）。MLP可以解决XOR问题</div><img src="Chapter 10. Introduction to Artificial Neural_files/2MQN64AUL0RGW2TYOL{CFH8.png" type="image/png" data-filename="2MQN64AUL0RGW2TYOL{CFH8.png" style="--en-uploadstate:uploaded;" width="464px"/><div>note：<span style="background-color: #ffffff;">将特征矩阵的每行（一个训练实例）喂给了神经网络，</span>信号只在一个方向上流动（从输入到输出），所以这种结构是<b>前馈神经网络（FNN）</b>的一个例子；当一个人工神经网络包含一个深层的隐藏层堆栈时，它被称为<b>深度神经网络（DNN）</b>，深度学习领域研究dnns，以及更普遍的包含深度计算堆栈的模型。即便如此，每当涉及到神经网络时（甚至是浅层的神经网络），许多人都会谈论深度学习。</div><img src="Chapter 10. Introduction to Artificial Neural_files/O5YX22Z5)Y3LG02TUY]XL7H.png" type="image/png" data-filename="O5YX22Z5)Y3LG02TUY]XL7H.png" style="--en-uploadstate:uploaded;" width="582px"/><div><span style="font-size: 14pt;"><b>（2）</b></span> <span style="font-size: 14pt;">backpropagation algorithm 反向传播算法</span></div><div>Let’s run through this algorithm in a bit more detail:</div><ol><li><div>It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an<span style="font-weight: bold;"><i>epoch</i></span>.它一次处理一个小批处理（例如，每个实例包含32个实例），并多次通过完整的训练集。每个 都被称为一个时代。</div></li><li><div>Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to thenext layer, and so on until we get the output of the last layer, the outputlayer. This is the<span style="font-weight: bold;"><i>forward pass</i></span>: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.每个小批处理被传递到网络的输入层，该输入层将其发送到第一个隐藏层。然后，该算法计算这一层中所有神经元的输出（对于小批处理中的每个实例）。结果被传递到下一层，它的输出被计算并传递到下一层，以此类推，直到我们得到最后一层的输出，即输出层。这是<span style="color: #FF0000;">正向传递</span>：它就像做预测一样，<span style="background-color: #fff199;">除了它还会把所有的中间结果都被保留了下来</span>，因为它们是反向传递所需要的。</div></li><li><div>Next, the algorithm measures the network's output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).接下来，该算法<span style="background-color: #fff199;">度量网络的输出误差</span>（即，它使用一个损失函数来比较期望的输出和网络的实际输出，并返回一些误差的度量）。</div></li><li><div>Then it computes how much each output connection contributed to the error. This is done analytically by applying the <span style="font-weight: bold;"><i>chain rule </i></span>(perhaps the most fundamental rule in calculus), which makes this step fast and precise.然后，它<span style="background-color: #fff199;">计算每个输出连接对错误的贡献程度</span>。这是通过应用链式规则（可能是微积分中最基本的规则）来解析地完成的，这使得这一步快速而精确。</div></li><li><div>The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).然后，<b>算法测量有多少这些错误贡献来自下面一层中的每个连接，同样使用链规则，向后工作，直到算法到达输入层</b>。如前所述，这种反向传递通过通过网络反向传播误差梯度（因此称为算法），有效地测量了网络中所有连接权值的误差梯度的测量。</div></li><li><div>Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.最后，该算法利用刚刚计算出的误差梯度，执行一个梯度下降步骤来调整网络中的所有连接权值。</div></li></ol><div><br/></div><div>这个算法是如此重要，值得再次总结：<b>对于每个训练实例，反向传播算法首先进行预测（向前通过）和测量误差</b>，<span style="background-color: #fff199;">然后通过每一层反向测量每个连接的误差贡献（反向通过），最后调整连接权重以减少误差（<b>梯度下降步骤</b>）</span></div><div><br/></div><ul><li><p><span style="background-color: #ffffff;">利用正向传播方法计算每层的激活单元</span></p></li><li><p><span style="background-color: #ffffff;">利用训练集的真实结果与神经网络的预测结果求出最后一层的误差</span></p></li><li><p><span style="background-color: #ffffff;">最后利用该误差运用反向传播法计算出直至第二层的所有误差。</span></p></li><li><p><span style="background-color: #ffffff;">便可以计算代价函数的偏导数</span></p></li></ul><div><br/></div><div><br/></div><div>warning：<b>随机初始化所有隐藏层的连接权值是很重要的</b>，否则训练就会失败。例如，如果你将所有的权值和偏差初始化为零，那么给定层中的所有神经元都将是完全相同的，因此反向传播将以完全相同的方式影响它们，所以它们将保持相同。换句话说，尽管每层有数百个神经元，但你的模型会表现得好像每层只有一个神经元：它不会太聪明。</div><div><br/></div><div>具体例子及数学+代码分析</div><div><a href="https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247509124&amp;idx=2&amp;sn=e691dd27062805f6907cb277fd141462&amp;chksm=976c0119a01b880fcacbabda278815942cb4feecee748c98332844176549e4fe689c11f3ebec&amp;scene=21#wechat_redirect" rev="en_rl_none">吴恩达《Machine Learning》精炼笔记 5：神经网络 (qq.com)</a></div><div><br/></div><ul><li><div>激活函数</div></li></ul><div>为了使这个算法正常工作，它的作者对MLP的架构做了一个关键的改变：他们用逻辑函数取代了阶跃函数。这是至关重要的，因为步骤函数只包含平面段，<b>所以没有梯度来工作（梯度下降不能在平面上移动），而逻辑函数到处都有一个定义良好的非零导数，允许梯度下降在每一步都取得一些进展</b>。事实上，反向传播算法可以很好地应用于许多其他激活函数，而不仅仅是逻辑函数。以下是另外两个流行的选择：</div><div><br/></div><div>如果层之间没有一些非线性，那么即使是一个很深的层堆栈也相当于一个层，你不能这样解决非常复杂的问题。相反地，一个足够大的具有<span style="background-color: #fff199;">非线性激活的DNN在理论上可以近似于任何连续函数</span>。</div><div><a href="https://zhuanlan.zhihu.com/p/364620596" rev="en_rl_none">深度学习笔记：如何理解激活函数？（附常用激活函数） - 知乎 (zhihu.com)</a></div><img src="Chapter 10. Introduction to Artificial Neural_files/~MW6GHUYYI7J@@)K3]M~F46.png" type="image/png" data-filename="~MW6GHUYYI7J@@)K3]M~F46.png" style="--en-uploadstate:uploaded;" width="580px"/><div><br/></div><div><span style="font-weight: bold;">Regression MLPs</span></div><img src="Chapter 10. Introduction to Artificial Neural_files/@I0L8G]EGQPP{GJ6`@M00SN.png" type="image/png" data-filename="@I0L8G]EGQPP{GJ6`@M00SN.png" style="--en-uploadstate:uploaded;" width="534px"/><div><span style="font-weight: bold;">Classification MLPs</span></div><div>这里可以解决二分类问题，多标签二分类问题，多分类问题（如下），一个普通的网络结构图如下:</div><img src="Chapter 10. Introduction to Artificial Neural_files/Image [2].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="499px"/><div>您需要<span style="background-color: #fff199;">每个类有一个输出神经元</span>，并且您应该对整个输出层使用softmax激活函数。<span style="background-color: #fff199;">softmax函数</span>（在第4章中介绍）将确保所有估计的概率都在0到1之间，并且它们加起来为1（如果类是独占的，这是必需的）。这叫做多分类。</div><img src="Chapter 10. Introduction to Artificial Neural_files/Image [3].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;"/><div><br/></div><div><br/></div><h2 style="text-align:start;">Implementing MLPs with Keras</h2><div><span style="background-color: #fff199;">Keras</span> is a high-level Deep Learning API that allows you to easily build, train, evaluate, and execute all sorts of neural networks.</div><img src="Chapter 10. Introduction to Artificial Neural_files/Image [4].png" type="image/png" data-filename="Image.png" style="--en-uploadstate:uploaded;" width="591"/><div><br/></div></span>
</div></body></html> 